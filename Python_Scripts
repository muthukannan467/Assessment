2. Python Scripts

`data_pipeline.py`

```python
import pandas as pd
import numpy as np
import logging
import requests
import mysql.connector
import psycopg2
from google.cloud import storage
from datetime import datetime

# Setup logging
logging.basicConfig(filename='data_pipeline.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Define constants
MYSQL_CONFIG = {
    'user': 'mysql_user',
    'password': 'mysql_password',
    'host': 'mysql_host',
    'database': 'mysql_db'
}

POSTGRES_CONFIG = {
    'user': 'postgres_user',
    'password': 'postgres_password',
    'host': 'postgres_host',
    'database': 'postgres_db'
}

GCS_BUCKET_NAME = 'your_bucket_name'
REST_API_URL = 'https://api.example.com/products'
MESSAGING_API_URL = 'https://api.example.com/whatsapp_registrations'

# Data Collection Functions
def collect_postgres_data():
    try:
        conn = psycopg2.connect(**POSTGRES_CONFIG)
        query = "SELECT * FROM user_transactions;"
        data = pd.read_sql(query, conn)
        conn.close()
        logging.info("Data collected from PostgreSQL.")
        return data
    except Exception as e:
        logging.error(f"Error collecting data from PostgreSQL: {e}")
        raise

def collect_gcs_data():
    try:
        client = storage.Client()
        bucket = client.bucket(GCS_BUCKET_NAME)
        blob = bucket.blob('web_logs.csv')
        blob.download_to_filename('web_logs.csv')
        data = pd.read_csv('web_logs.csv')
        logging.info("Data collected from Google Cloud Storage.")
        return data
    except Exception as e:
        logging.error(f"Error collecting data from GCS: {e}")
        raise

def collect_rest_api_data():
    try:
        response = requests.get(REST_API_URL)
        data = pd.json_normalize(response.json())
        logging.info("Data collected from REST API.")
        return data
    except Exception as e:
        logging.error(f"Error collecting data from REST API: {e}")
        raise

def collect_mysql_data():
    try:
        conn = mysql.connector.connect(**MYSQL_CONFIG)
        query = "SELECT * FROM web_registrations;"
        data = pd.read_sql(query, conn)
        conn.close()
        logging.info("Data collected from MySQL.")
        return data
    except Exception as e:
        logging.error(f"Error collecting data from MySQL: {e}")
        raise

def collect_whatsapp_data():
    try:
        response = requests.get(MESSAGING_API_URL)
        data = pd.json_normalize(response.json())
        logging.info("Data collected from WhatsApp API.")
        return data
    except Exception as e:
        logging.error(f"Error collecting data from WhatsApp API: {e}")
        raise

# Data Processing Functions
def process_data():
    try:
        transactions = collect_postgres_data()
        web_logs = collect_gcs_data()
        product_data = collect_rest_api_data()
        web_registrations = collect_mysql_data()
        whatsapp_registrations = collect_whatsapp_data()
        
        # Data Processing
        user_registrations = pd.concat([web_registrations, whatsapp_registrations], ignore_index=True)
        user_registrations.drop_duplicates(subset=['user_id'], keep='first', inplace=True)
        
        # Data Enrichment
        enriched_data = transactions.merge(user_registrations, on='user_id', how='left')
        enriched_data = enriched_data.merge(product_data, on='product_id', how='left')
        
        # Data Quality Check
        if enriched_data.isnull().sum().sum() > 0:
            logging.warning("Data contains null values.")
        
        logging.info("Data processed successfully.")
        return enriched_data
    except Exception as e:
        logging.error(f"Error processing data: {e}")
        raise

# Data Storage Function
def store_data(data):
    try:
        from google.cloud import bigquery
        client = bigquery.Client()
        table_id = 'your_project.your_dataset.processed_data'
        job = client.load_table_from_dataframe(data, table_id)
        job.result()  # Waits for the job to complete
        logging.info("Data stored in Google BigQuery.")
    except Exception as e:
        logging.error(f"Error storing data: {e}")
        raise

def main():
    try:
        processed_data = process_data()
        store_data(processed_data)
    except Exception as e:
        logging.error(f"Pipeline failed: {e}")
        raise

if __name__ == "__main__":
    main()
```
